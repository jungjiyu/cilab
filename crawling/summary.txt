
: vs 코드 extenstion 설치 >> 주피터 + 파이썬 
	: 실행 >> ctrl + enter
	: 다음 셀을 만들면서 실행 >> shift + enter
	: 다음 셀과 조인 >> alt + windows + j


: python 설치 >> 3.12.3 설치


파이썬 문자열 조작 tip
	: 문자열 중간에 변수 삽입하기 >> f" 쏼라쏼라 { 변수명1 } ... {변수명n } 쏼라쏼라  '
	: 유용 메서드
		(0) .split( '특정문자열' ) : 특정문자열을 기준으로 원본문자열을 2분할한 리스트를 반환한다 << 해당 특정문자열은 반환 결과에 포함되지 않는다 
			ex ) '1,419,000원\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t'.split('원') == ['1,419,000', '\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t' ]
		(1) .strip( ) : 앞뒤의 화이트스페이스(\t , \n 등의) 제거
		(2) .replace( '변경되는타겟', '새로삽입될내용' ) : 해당 부분을 새로운 표현으로 대체
			: 만약 특정 부분을 없애고 싶다면 걍  .replace( '변경되는타겟', '' ) 하면 됨. 




크롤링 기초
	: https://youtu.be/Uf21RUo3KNc?si=YzsWN8a_8qBV9ffh
		: 영상에서 크롤링 연습하는 사이트 >> https://startcoding.pythonanywhere.com/

	: 웹 크롤링 >> Web 거미줄 + Crawling 기어다니다 . 거미줄 기어다니는 것처럼 웹 상의 정보를 수집한다
		: 웹 스크래핑 >> web + scraping 긁어모으다 . 웹 크롤링과 같은 단어는 아니지만 , 혼용되어 자주 쓰임
	: 활용 >> 방대한 데이터 수집용
	: 정적 페이지 static page 크롤링 
		: 정적페이지 >> 데이터의 추가적인 변경이 안일어나는 페이지. 
		: 크롤링 
		1. 데이터 받아오기 >>  파이썬에서 요청을 보내고 응답 받기
			: requests 라이브러리 >> HTTP 통신으로 HTML 을 받아온다
			    : 다운로드 >> pip install requests
			    : 요청하기 >> requests.HTTP요청방식("요청할주소")
				: 응답이 반환되고, 이걸 변수로 받아둔다. 
					ex) response = requests.get("https://startcoding.pythonanywhere.com/basic");
				: 반환 받은 응답 객체의 속성
					1. status_code : 상태코드값 
					2. text : 반환받은 html 문서. 문자열 타입.
						: 데이터는 이 속성을 통해 받아두고, 파싱 자체는 BeautifulSoup4 를 이용하여 파싱한다. ( 문자열 자체로는 추출이 어려워서 )

		2. 데이터 뽑아오기 >> 받은 응답에서 원하는 부분만 추출하기
			: css 선택자를 잘 만드는 것이 핵심
			: BeautifulSoup4 라이브러리 
				: 다운로드 >> pip install bs4
				: 문자열을 파싱하기 >>앞서 응답받았던 문서를 태그 객체로 파싱한다
					: 앞서 반환받았던 html 문서를 BeautifulSoup( 파싱당할문자열 , '파싱주체' )  메서드를 활용하여 파싱한다. 파싱 주체는 html 문서의 경우 'html.parser' 로 한다.
					ex ) 
					html = response.text
					soup = BeautifulSoup(html, 'html.parser')
				: (BeautifulSoup( 문자열 , 'html.parser') 로 ) 반환받은 파싱 객체를 대상으로 요소 객체들을 뽑아주는 메서드
					1. select_one("css선택자") : 해당 선택자와 "첫번째로 매칭되는" 객체를 반환
						: id 면 #, 클래스면 . 같은거 붙이는거 잊음 안된다

					2. select("css선택자") : 해당 선택자와 매칭되는 모든 객체"들"을 "리스트"로 반환
			

				: ( select_one("선택자")로 )반환받은 요소 객체의 속성
					: 아마 script 문에서의 요소 객체랑 비슷한 것 같다
					1. .text >> 텍스트 부분만 뽑아낸다. 아마 innerHTML 부분인듯
					2. attrs >> 해당 html요소의 속성들을 요소로하는 "딕셔너리"를 반환한다
						: 특정 속성으로의 접근을 위해서는  attrs['특정속성명'] 을 사용하면 된다

	: 동적 페이지 크롤링
		: 데이터의 추가적인 변경이 일어나는 페이지.


  
여러개의 상품 크롤링 하는 법 << 포레스트 이론. 숲에서 원하는 나무를 선택하고 원하는 열매를 따는 것과 비슷
	: 숲 - 해당 HTML 문서 전체
	: 나무 - 원하는 정보를 모두 담는 태그<< 나무 역할의 태그는 여러개가 될 수 있다
	: 방법
		1. 나무 태그(들)을 정하고, 해당 요소 객체를 구한다 >> soup 객체를 대상으로 .select('선택자') 사용
			: 나무 태그 정하는 법 >> 원하는 정보들을 담는 태그들을 찾고, 상위로 올라가면서 해당 태그들을 모두 포함하는 가장 근접한 태그를 찾는다

		2. 반복문을 돌면서 해당 나무에서 열매를 하나씩 추출한다

	: ex ) 
response = requests.get("https://startcoding.pythonanywhere.com/basic")
html = response.text
soup = BeautifulSoup(html, 'html.parser')
items = soup.select('.product')
for item in items :
    name = item.select_one(".product-name").text
    category = item.select_one(".product-category").text
    rawPrice = item.select_one(".product-price").text
    price = item.select_one(".product-price").text.strip().replace(',','').replace('원','')
    print(name,category,price)



여러 페이지 크롤링 하는 법 >> request 하는 URL  뒤에 파라미터 같은걸 잘 주면된다	
	: reuqest 자체를 반복문 돌려서 페이지 단위로 여러개 하면 된다. 그래고 url 은 fstring 을 사용하여 변화를 시켜준다.


데이터 엑셀에 저장하는 법
	: pandas 와 openpyxl 라이브러리 활용
	: pandas 라이브러리 >> 데이터 프레임 생성
		다운로드 : pip install pandas
		데이터 프레임 : (행과 열이 있는) 표 형태의 데이터 구조
		    : 생성하는 방법 : pandas라이브러리명.DataFrame( 데이터프레임화대상, columns= [ '컬럼명1', '컬럼명2' , '컬럼명3' , ... ])
			: 만약 걍 import 했으면 pandas 그대로 쓰면 되겠지만,  import pandas as pd 이런식으로 import 했었다면 pd.DataFrame 사용하면 된다.
			: 2번째 arg 로 들어가는 columns= 에는 생성할 데이터 프레임을 구성할 컬럼명을 정의해준다
			: 프레임화대상은 주로 (크롤링데이터를담은) 2차원 리스트 타입의 객체
				: 그러니까 프레임화대상구하는법
				  1. 크롤링 반복문 돌리기 이전에 리스트를 선언한다
				  2. 크롤링 반복문 돌리면서 해당 리스트에 데이터를 append 한다
				  3. 반복문이 끝나고, 그 리스트객체를 DataFrame 의 arg 로 한다
				: ex )
					data = [] # 1. 크롤링 데이터 담을 리스트 선언
					for i in range(1,5):
					    response = requests.get(f"https://startcoding.pythonanywhere.com/basic?page={i}")
					    html = response.text
					    soup = BeautifulSoup(html, 'html.parser')
					    items = soup.select('.product')
					    for item in items :
					        name = item.select_one(".product-name").text
					        category = item.select_one(".product-category").text
					        rawPrice = item.select_one(".product-price").text
					        price = item.select_one(".product-price").text.split('원')[0].replace(',','')
					        print(name,category,price)
					        data.append([name,category,price]) #2. 리스트에 데이터 append

					df = pd.DataFrame(data, columns=['이름','카테고리','가격']) # 3. 정리한 데이터 리스트를 DataFrame 의 arg로 한다

		    : 활용 >> 해당 객체를 대상으로, 해당 데이터를 다른 형식의 파일로 변환할 수 있다.
			: 데이터 셋을 엑셀화 하기 >> 데이터셋객체.to_excel('파일명.xslx')

					
	: openyxl 라이브러리 >> 데이터 프레임을 엑셀화 하는데 사용된다
		다운로드 : pip install openyxl
		    : 참고로 다운로드만 함 되지 , 별도로 import 할 필요는 없다
		활용 : 직접적으로 명시하진 않고, 데이터 프레임객체를 대상으로 하는 메서드( .to_excel 같은)를 활용할 때, 보이진 않지만 내부적으로 사용된다




