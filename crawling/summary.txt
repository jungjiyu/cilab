
: vs 코드 extenstion 설치 >> 주피터 + 파이썬 
	: 실행 >> ctrl + enter
	: 다음 셀을 만들면서 실행 >> shift + enter
	: 다음 셀과 조인 >> alt + windows + j


: python 설치 >> 3.12.3 설치


파이썬 문자열 조작 tip
	: 문자열 중간에 변수 삽입하기 >> f" 쏼라쏼라 { 변수명1 } ... {변수명n } 쏼라쏼라  '
	: 유용 메서드
		(0) .split( '특정문자열' ) : 특정문자열을 기준으로 원본문자열을 2분할한 리스트를 반환한다 << 해당 특정문자열은 반환 결과에 포함되지 않는다 
			ex ) '1,419,000원\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t'.split('원') == ['1,419,000', '\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t' ]
		(1) .strip( ) : 앞뒤의 화이트스페이스(\t , \n 등의) 제거
		(2) .replace( '변경되는타겟', '새로삽입될내용' ) : 해당 부분을 새로운 표현으로 대체
			: 만약 특정 부분을 없애고 싶다면 걍  .replace( '변경되는타겟', '' ) 하면 됨. 




크롤링 기초
	: https://youtu.be/Uf21RUo3KNc?si=YzsWN8a_8qBV9ffh
		: 영상에서 크롤링 연습하는 사이트 >> https://startcoding.pythonanywhere.com/

	: 웹 크롤링 >> Web 거미줄 + Crawling 기어다니다 . 거미줄 기어다니는 것처럼 웹 상의 정보를 수집한다
		: 웹 스크래핑 >> web + scraping 긁어모으다 . 웹 크롤링과 같은 단어는 아니지만 , 혼용되어 자주 쓰임
	: 활용 >> 방대한 데이터 수집용
	: 정적 페이지 static page 크롤링 
		: 정적페이지 >> 데이터의 추가적인 변경이 안일어나는 페이지. 
		: 크롤링 
		1. 데이터 받아오기 >>  파이썬에서 요청을 보내고 응답 받기
			: requests 라이브러리 >> HTTP 통신으로 HTML 을 받아온다
			    : 다운로드 >> pip install requests
			    : 요청하기 >> requests.HTTP요청방식("요청할주소")
				: 응답이 반환되고, 이걸 변수로 받아둔다. 
					ex) response = requests.get("https://startcoding.pythonanywhere.com/basic");
				: 반환 받은 응답 객체의 속성
					1. status_code : 상태코드값 
					2. text : 반환받은 html 문서. 문자열 타입.
						: 데이터는 이 속성을 통해 받아두고, 파싱 자체는 BeautifulSoup4 를 이용하여 파싱한다. ( 문자열 자체로는 추출이 어려워서 )

		2. 데이터 뽑아오기 >> 받은 응답에서 원하는 부분만 추출하기
			: css 선택자를 잘 만드는 것이 핵심
			: BeautifulSoup4 라이브러리 
				: 다운로드 >> pip install bs4
				: 문자열을 파싱하기 >>앞서 응답받았던 문서를 태그 객체로 파싱한다
					: 앞서 반환받았던 html 문서를 BeautifulSoup( 파싱당할문자열 , '파싱주체' )  메서드를 활용하여 파싱한다. 파싱 주체는 html 문서의 경우 'html.parser' 로 한다.
					ex ) 
					html = response.text
					soup = BeautifulSoup(html, 'html.parser')
				: (BeautifulSoup( 문자열 , 'html.parser') 로 ) 반환받은 파싱 객체를 대상으로 요소 객체들을 뽑아주는 메서드
					1. select_one("css선택자") : 해당 선택자와 "첫번째로 매칭되는" 객체를 반환
						: id 면 #, 클래스면 . 같은거 붙이는거 잊음 안된다

					2. select("css선택자") : 해당 선택자와 매칭되는 모든 객체"들"을 "리스트"로 반환
			

				: ( select_one("선택자")로 )반환받은 요소 객체의 속성
					: 아마 script 문에서의 요소 객체랑 비슷한 것 같다
					1. .text >> 텍스트 부분만 뽑아낸다. 아마 innerHTML 부분인듯
					2. attrs >> 해당 html요소의 속성들을 요소로하는 "딕셔너리"를 반환한다
						: 특정 속성으로의 접근을 위해서는  attrs['특정속성명'] 을 사용하면 된다

	: 동적 페이지 크롤링
		: 데이터의 추가적인 변경이 일어나는 페이지.


  
여러개의 상품 크롤링 하는 법 << 포레스트 이론. 숲에서 원하는 나무를 선택하고 원하는 열매를 따는 것과 비슷
	: 숲 - 해당 HTML 문서 전체
	: 나무 - 원하는 정보를 모두 담는 태그<< 나무 역할의 태그는 여러개가 될 수 있다
	: 방법
		1. 나무 태그(들)을 정하고, 해당 요소 객체를 구한다 >> soup 객체를 대상으로 .select('선택자') 사용
			: 나무 태그 정하는 법 >> 원하는 정보들을 담는 태그들을 찾고, 상위로 올라가면서 해당 태그들을 모두 포함하는 가장 근접한 태그를 찾는다

		2. 반복문을 돌면서 해당 나무에서 열매를 하나씩 추출한다

	: ex ) 
response = requests.get("https://startcoding.pythonanywhere.com/basic")
html = response.text
soup = BeautifulSoup(html, 'html.parser')
items = soup.select('.product')
for item in items :
    name = item.select_one(".product-name").text
    category = item.select_one(".product-category").text
    rawPrice = item.select_one(".product-price").text
    price = item.select_one(".product-price").text.strip().replace(',','').replace('원','')
    print(name,category,price)



여러 페이지 크롤링 하는 법 >> request 하는 URL  뒤에 파라미터 같은걸 잘 주면된다	
	: reuqest 자체를 반복문 돌려서 페이지 단위로 여러개 하면 된다. 그래고 url 은 fstring 을 사용하여 변화를 시켜준다.


데이터 엑셀에 저장하는 법
	: pandas 와 openpyxl 라이브러리 활용
	: pandas 라이브러리
		: 다운로드 >> pip install pandas
	: openyxl 라이브러리
		: 다운로드 >> pip install openyxl



