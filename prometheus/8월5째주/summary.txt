todo

1. 과부하 어느정도에서 걸리나 정도

2. 모니터링 실전
	: vLLM >> 속도 개선
	: groq >> 여기 서비스 가입하면 무료로 API 호출할수 있는 key 얻을 수 있음
	: chatgpt || 라마 || 젬마 >>글 완성해줘,  그림 설명해줘 관련 API 호출
	: 서비스 쪽에서 어케 사용자 데이터(키 등) 넘겨받을것인가?


------------------------------------------------------------------------------------------------------

LLM Large Language Model 대규모 언어모델
 
Time to First Token (TTFT) >>  첫 번째 토큰이 생성되기까지의 시간

Context Windows 컨텍스트 윈도우 >> 시스템이 한 번에 처리할 수 있는 정보의 양
	: 주로 LLM 분야에서 토큰 수를 의미

LPU Language Processing Unit 언어 처리 장치 >>  LLM에 특화된 칩(하드웨어)
	: (현시점 여전히) 시장 대부분의 (GPT나 Gemini와 같은) LLM은 프라이빗 서버나 클라우드에 배포된 (엔비디아 같은 회사에서 공급하는) GPU(그래픽 프로세서)를 활용하는데, 이는 GPU 아키텍처에 의존한다는 점에서 LLM 추론에 최적이 아니다.
		: LPU 는 GPU에 비해 전반적으로 뛰어난 성능(낮은 지연 시간, 높은 처리 성능.. )을 보인다.
			: LPU는 TTFT가 0.22초 밖에 안된다
			: GPU 는 동시에 여러 작업을 처리 가능한데, 작업량에 따라 응답 속도가 달라질 수 있다(대기시간이 길어질 수 있다). 반면, LPU는 모든 작업을 동일한 시간 내에 정확하게 실행함으로써, 일관되고 신속한 처리를 한다.  
				: Groq가 개발한 이 방식을 TSP Tensor Streaming Processor 아키텍처라고 한다
				: TSP는 GPU에 비해 낮은 복잡성을 가진다



groq >> LPU 개발한 회사
	: https://meetcody.ai/ko/blog/%EA%B7%B8%EB%A3%A8%ED%81%AC%EC%99%80-%EB%9D%BC%EB%A7%88-3-%ED%8C%90%EB%8F%84%EB%A5%BC-%EB%B0%94%EA%BE%B8%EB%8A%94-%EB%93%80%EC%98%A4/
	: https://yunwoong.tistory.com/310 

	: groq cloud (groq playground) >> LPU를 사용하는 다양한 모델을 사용 가능하다
		: 지원 모델
			1. Llama 3.1 시리즈( Llama 3.1 405B, 70B, 8B ): Meta에서 개발한 모델들
				: 큰 컨텍스트 윈도우(최대 131,072 토큰)를 제공하며, 다국어 번역, 도구 사용, 일반 지식 질의 등의 복잡한 작업을 처리 가능.

			2. Gemma 시리즈( Gemma 2 9B, Gemma 7B ) : Google에서 개발한 모델들						: 최대 8,192 토큰의 컨텍스트 윈도우를 제공

			3. Mixtral 8x7B: Mistral에서 개발한 모델
				: 2,768 토큰의 컨텍스트 윈도우를 제공

			4. Whisper 및 Distil-Whisper: 음성 전사 작업을 위한 모델
				: 다양한 언어를 지원

		: how to use
			1. groq 회원가입 << 나는 구글
			2. 사용할 모델 선택 
			3. 활용 >> Prompt 입력하거나 API 키 사용하거나


	: 일정 사용량 까지는 무료

------------------------------------------------------------------------------------------------------


(AI에서의) 추론 inference >> 학습된 지식을 활용하여 가장 적절한 답변을 예측하는 과정
	
(AI에서의) 서빙 serving >>  LLM을 실제 애플리케이션에 통합하고 사용자에게 실시간으로 서비스를 제공하기 위한 전체적인 과정
	: 다음 3가지 주요 작업을 포괄함( 서빙 == 베포 + 추론 + API 제공  )
		1. 모델 배포 Deployment :  LLM을 클라우드 또는 온프레미스 서버에 배표
		2. 실시간 추론 Inference :  사용자가 입력한 텍스트에 대해 배포된 모델이 즉각적으로 결과를 생성하여 반환
		3. API 제공:  API를 통한 접근을 제공하여, 모델이 다양한 애플리케이션에서 쉽게 통합되고 활용될 수 있게함



vLLM >>  LLM 추론 및 서빙을 쉽게, 빠른 속도로 가능하게 하는 라이브러리
	; vLLM 의 코드 분석 ( TMI ) >> https://tech.scatterlab.co.kr/vllm-implementation-details/
	: vLLM 전반적인 설명 >> https://lsjsj92.tistory.com/668
	: LLM을 쉽고 빠르게 deploy(배포), inference(추론), serving(서빙)할 수 있는 라이브러리

