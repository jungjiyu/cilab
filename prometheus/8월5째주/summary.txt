todo

1. 과부하 어느정도에서 걸리나 정도 >> Jmeter 올리면서 test 함

2. 모니터링 실전 >> chatgpt/Gemma/Lamma api를 호출하여 글 완성해줘,  그림 설명해줘 등을 프롬프트로하여 API를 호출하는 스프링부트 애플리케이션을 만들고 서버에 배포하되, 이 과정에서  vLLM과 groq를 활용
    : 계획
	:  텍스트 처리에 vLLM을 사용한 이유 >> 
		: 이미지 처리 
			:  CNN과 같은 구조를 통해 (이미지) 데이터를 분석한다.
			:  ResNet, EfficientNet, Vision Transformer(ViT)와 같은 모델이 일반적으로 사용된다.

		: vLLM 자체가 자연어 처리(NLP) 작업(텍스트 처리)에 특화되어있다. 이미지 처리에는 잘 안쓴다. 
		: Groq의 LPU는 고성능 처리에 특화되어있으나 현재 API 로 제공하는 모델들 중에는 이미처리를 할 수 있는 모델이 없으므로 얘도 일단 텍스트 처리에 써보자.



	: vLLM >> 서버에 vLLM을 설치하여 GPT-3, LLaMA, Gemma 모델을 서빙하고, 스프링부트 애플리케이션에서 이 모델들을 호출해 "텍스트 완성"에 활용
		: vLLM을 Docker 컨테이너를 활용해 배포해서 활용함 된다.

	: groq >> 스프링부트 애플리케이션에서 Groq API를 통해 LPU 기반 모델을 호출해 "이미지 설명"에 활용
		: 서버에 물리적인 LPU 칩은 없는 상황이라, Groq에서 제공하는 API를 통해 LPU 기반의 모델을 원격으로 호출하는 정도로 활용.


3. 서비스 쪽에서 어케 사용자 데이터(키 등) 넘겨받을것인가? >> knlab MSA 구조 활용함 될 듯



    : 요약
	: vLLM >> LLM의 추론 및 서빙을 쉽고 빠르게하는 라이브러리
	: groq >> 여기 서비스 가입하면 무료로 API 호출할수 있는 key 얻을 수 있음
	: chatgpt || 라마 || 젬마 >>글 완성해줘,  그림 설명해줘 관련 API 호출
	: 서비스 쪽에서 어케 사용자 데이터(키 등) 넘겨받을것인가?


------------------------------------------------------------------------------------------------------
<groq 이론>


LLM Large Language Model 대규모 언어모델
 
Time to First Token (TTFT) >>  첫 번째 토큰이 생성되기까지의 시간

Context Windows 컨텍스트 윈도우 >> 시스템이 한 번에 처리할 수 있는 정보의 양
	: 주로 LLM 분야에서 토큰 수를 의미

LPU Language Processing Unit 언어 처리 장치 >>  LLM에 특화된 칩(하드웨어)
	: (현시점 여전히) 시장 대부분의 (GPT나 Gemini와 같은) LLM은 프라이빗 서버나 클라우드에 배포된 (엔비디아 같은 회사에서 공급하는) GPU(그래픽 프로세서)를 활용하는데, 이는 GPU 아키텍처에 의존한다는 점에서 LLM 추론에 최적이 아니다.
		: LPU 는 GPU에 비해 전반적으로 뛰어난 성능(낮은 지연 시간, 높은 처리 성능.. )을 보인다.
			: LPU는 TTFT가 0.22초 밖에 안된다
			: GPU 는 동시에 여러 작업을 처리 가능한데, 작업량에 따라 응답 속도가 달라질 수 있다(대기시간이 길어질 수 있다). 반면, LPU는 모든 작업을 동일한 시간 내에 정확하게 실행함으로써, 일관되고 신속한 처리를 한다.  
				: Groq가 개발한 이 방식을 TSP Tensor Streaming Processor 아키텍처라고 한다
				: TSP는 GPU에 비해 낮은 복잡성을 가진다



groq >> LPU 개발한 회사
	: https://meetcody.ai/ko/blog/%EA%B7%B8%EB%A3%A8%ED%81%AC%EC%99%80-%EB%9D%BC%EB%A7%88-3-%ED%8C%90%EB%8F%84%EB%A5%BC-%EB%B0%94%EA%BE%B8%EB%8A%94-%EB%93%80%EC%98%A4/
	: https://yunwoong.tistory.com/310 

	: groq cloud (groq playground) >> LPU를 사용하는 다양한 모델을 사용 가능하다
		: 지원 모델
			1. Llama 3.1 시리즈( Llama 3.1 405B, 70B, 8B ): Meta에서 개발한 모델들
				: 큰 컨텍스트 윈도우(최대 131,072 토큰)를 제공하며, 다국어 번역, 도구 사용, 일반 지식 질의 등의 복잡한 작업을 처리 가능.

			2. Gemma 시리즈( Gemma 2 9B, Gemma 7B ) : Google에서 개발한 모델들
				: 최대 8,192 토큰의 컨텍스트 윈도우를 제공

			3. Mixtral 8x7B: Mistral에서 개발한 모델
				: 2,768 토큰의 컨텍스트 윈도우를 제공

			4. Whisper 및 Distil-Whisper: 음성 전사 작업을 위한 모델
				: 다양한 언어를 지원

		: how to use
			1. groq 회원가입 << 나는 구글
			2. 사용할 모델 선택 
			3. 활용 >> Prompt 입력하거나 API 키 사용하거나


	: 일정 사용량 까지는 무료

------------------------------------------------------------------------------------------------------
<vLLM 이론>

(AI에서의) 추론 inference >> 학습된 지식을 활용하여 가장 적절한 답변을 예측하는 과정
	
(AI에서의) 서빙 serving >>  LLM을 실제 애플리케이션에 통합하고 사용자에게 실시간으로 서비스를 제공하기 위한 전체적인 과정
	: 다음 3가지 주요 작업을 포괄함( 서빙 == 베포 + 추론 + API 제공  )
		1. 모델 배포 Deployment :  LLM을 클라우드 또는 온프레미스 서버에 배표
		2. 실시간 추론 Inference :  사용자가 입력한 텍스트에 대해 배포된 모델이 즉각적으로 결과를 생성하여 반환
		3. API 제공:  API를 통한 접근을 제공하여, 모델이 다양한 애플리케이션에서 쉽게 통합되고 활용될 수 있게함



vLLM  Versatile Large Language Model >>  LLM 추론 및 서빙을 쉽게, 빠른 속도로 가능하게 하는 라이브러리
	; vLLM 의 코드 분석 ( TMI ) >> https://tech.scatterlab.co.kr/vllm-implementation-details/
	: vLLM 전반적인 설명 >> https://lsjsj92.tistory.com/668
	: LLM을 쉽고 빠르게 deploy(배포), inference(추론), serving(서빙)할 수 있는 라이브러리
	: 특징
		1. state-of-the-art serving throughput을 보여줌. 즉, 서빙 처리 속도가 좋음
		2. 페이지 어텐션(page attention) 방법으로 key, value 메모리를 효과적으로 관리
		3. 입력으로 들어오는 요청(request)에 대해서 지속적인 배치(Continuous batching) 처리 가능
		4. 양자화(Quantization) : GPTQ, AWQ, FP8 KV Cache 등
		5. 허깅페이스(huggingface)와의 원활한 통합으로 인기 있는 LLM 모델을 사용할 수 있음
		6. 분산 추론(distributed inference) 지원

	: 단점 >> linux 운영체제에서만 사용 가능
	: how to use
	    case 0. hugging face를 기본적으로 활용
		0. 파이썬 가상환경 생성
			: 파이썬 기반의 코드를 실행할거라서 파이썬을 활용해야된다

		1. 해당 가상환경에 필요한 라이브러리 설치 
		    (1) transformers 라이브러리 설치 >>pip install transformers
			: hugging face 사용해서 트랜스포머 모델 받기 위해 필요
		    (2) vllm 라이브러리 설치 >> pip install vllm

		2. 커스텀 한 가상환경 아래에 실습폴더 생성 
		3. 예제 코드 작성 : vim 으로 transformers라이브러리와 vllm라이브러리를 사용하는 "어쩌구.py" 파일을 작성
		4. 예제 코드 실행>>  python3 어쩌구.py
	

	    case 1. API 형태로 배포




------------------------------------------------------------------------------------------------------
<vLLM 활용 실습>


transformers 라이브러리
	: 자연어 처리(NLP)와 관련된 작업을 수행하기 위해 Hugging Face에서 개발한 오픈 소스 Python 라이브러리
	: 트랜스포머(Transformer) 구조를 기반으로 하는 모델들을 지원
	: pipline 모듈을 포함
	    : pipline 모듈 >>   다양한 NLP 작업을 쉽게 수행할 수 있도록 설계된 함수


venv 라이브러리
	: python 가상환경 만드는데 사용
	: 기본적인 사용 방법
		(1) 가상환경 생성 >>   python3 -m venv [가상환경명]
		(2) 가상환경 활성화 >> source [가상환경명]/bin/activate
		(3) 가상환경 비활성화 >> deactivate



nvidia 드라이버와 CUDA 드라이버
	: nvidia 드라이버 >> 운영 체제와 GPU 간의 "기본적인" 상호작용을 가능하게 함. 
		: 운영 체제가 GPU를 인식하고 사용할 수 있도록 하는 데 필요한 모든 저수준(low-level) 소프트웨어를 제공
		: ( 운영 체제와 GPU 간 상호 작용을 위해선) 필수적으로 설치되어야됨
			: NVIDIA 드라이버 없이 다른 드라이버(ex : CUDA 드라이버)를 설치할 수 없음

	: CUDA 드라이버 >> GPU를 활용한 병렬 컴퓨팅(병렬 연산)을 지원하는 소프트웨어
		:  CUDA 드라이버는 과학적 계산, 머신러닝, 딥러닝, 데이터 분석 등 복잡한 작업에서 사용되는 GPU 가속 프로그램을 지원
		: NVIDIA 드라이버가 설치된 상태에서 CUDA 드라이버가 작동
		: 필수는 아님 
		: CUDA 드라이버는 NVIDIA 드라이버의 상위 레이어로, NVIDIA 드라이버가 GPU와의 저수준 통신을 관리하는 반면, CUDA 드라이버는 병렬 컴퓨팅 작업을 관리




실습 내용
	-1.  GPU 와 CUDA 드라이버 확인 << : vLLM은 GPU 기반의 실행을 전제로 설계된 시스템이기 때문에, GPU가 올바르게 설정되고 CUDA 드라이버가 설치되어 있어야 한다 
		1. gpu 의 모델 확인 >> lspci | grep -i nvidia
			:  시스템에 연결된 모든 NVIDIA GPU를 나열한다.
			: 아래와 같이 출력됬다
(myenv) user@workstation:~/myenv/vllm_project$ lspci | grep -i nvidia
01:00.0 VGA compatible controller: NVIDIA Corporation Device 2786 (rev a1)
01:00.1 Audio device: NVIDIA Corporation Device 22bc (rev a1)


		2. GPU 드라이버 버전 확인 >> nvidia-smi
		: 아래와 같이 출력됬다 << 그러니까 GPU 드라이버( NVIDIA 드라이버)가 설치되어있지 않아서 그런것.
(myenv) user@workstation:~/myenv/vllm_project$ nvidia-smi
NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.


	-0.5 nvidia 드라이버 설치 >>
		1. sudo apt-get update
		2.  sudo apt-get install nvidia-driver-560 
			:   sudo apt-get install nvidia-driver-<적절한NVIDIA 드라이버버전> 인데,  2786은 NVIDIA GeForce RTX 4070 GPU에 해당하고, 이 GPU에 적합한 드라이버 버전은 NVIDIA의 최신 Game Ready Driver이고, 이는  560.81 버전이다.,

		3. sudo reboot >> 시스템 재부팅
		4. 제대로 설치됬나 확인 >>  nvidia-smi
			: 아래와 같이 정상적으로 출력

Mon Aug 26 18:10:26 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 4070        Off |   00000000:01:00.0  On |                  N/A |
|  0%   38C    P8              7W /  200W |      41MiB /  12282MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A      1772      G   /usr/lib/xorg/Xorg                             18MiB |
|    0   N/A  N/A      1982      G   /usr/bin/gnome-shell                            7MiB |
+-----------------------------------------------------------------------------------------+




	0. 파이썬 가상환경 구축 << vLLM 설치할 때 pip 써야되서 파이썬 필요
	     : /home/user 디렉터리에 myenv라는 이름으로 생성함
		(0) 업데이트 >> sudo apt update
		(1) python3-venv 패키지 설치 >> sudo apt install python3-venv
		(2) 가상환경 생성 >> python3 -m venv myenv
		(3) 가상환경 활성화 >> source myenv/bin/activate

	0. 해당 가상환경 내에 vLLM 실습을 위한 디렉터리 생성 >> /home/user/myenv 디렉터리에 vllm_project 디렉터리 생성

	1. vLLM 설치 
		(1) transformers 라이브러리 설치 >>pip install transformers
			: hugging face 사용해서 트랜스포머 모델 받기 위해 필요
		(2) vllm 라이브러리 설치 >> pip install vllm

	2. 예제 코드 작성 << vim 을 활용
		:  vim example.py 하고 아래의 내용입력

		(1)  vLLM을 사용하지 않은(transformers 만 사용) 스크립트

from transformers import pipeline

# 텍스트 생성 파이프라인 생성
generator = pipeline('text-generation', model='gpt2')

# 텍스트 생성
result = generator("Once upon a time", max_length=50, num_return_sequences=1)

# 결과 출력
print(result)




		(2)  vLLM을 사용한 스크립트
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams

# GPT-2 모델과 토크나이저 로드
tokenizer = AutoTokenizer.from_pretrained("gpt2")

llm = LLM(model="gpt2", enforce_eager=True)

# 입력 텍스트 정의
input_text = "The future of AI is"

# 샘플링 파라미터 설정
sampling_params = SamplingParams(temperature=0.7, top_p=0.9)

# vLLM을 사용해 텍스트 생성
output = llm.generate([input_text], sampling_params=sampling_params)

# RequestOutput 객체에서 텍스트 추출
generated_text = output[0].outputs[0].text
print(generated_text)







	3. 예제 코드 실행 >> python3 example.py
		: 파이썬 실행시키는거라 가상환경 활성화된 상태에서 해야됨
		: vLLM을 사용하지 않은 경우와 사용한 경우의 실습 결과 비교
			: 결론
				높은 성능: 요청 처리 속도와 처리량이 개선
				메모리 효율성: GPU 메모리를 최적화하여 더 많은 요청을 처리 가능
				지연 시간 감소: 텍스트 생성 시 지연 시간이 감소
				사용 편의성: 복잡한 설정 없이도 효율적인 모델 서빙이 가능


			1. vLLM을 사용하지 않은 경우
			     (0) 성능
				: 모델이 단순하게 순차적으로 요청을 처리. 효율성이 낮을 수 있으며, 특히 대규모 모델을 여러 요청에 대해 처리할 때 성능 저하가 발생할 수 있다.
				: 모델 로딩 및 추론 시간이 더 오래 걸릴 수 있다

			     (1) 메모리 효율성 >> 각 요청이 개별적으로 처리되기 때문에 GPU 메모리 사용이 비효율적일 수 있고, 이로 인해 메모리 부족 문제가 발생할 가능성이 있다

			     (2) 지연 시간 Latency >>요청당 개별 추론이 수행되기 때문에 응답 지연 시간이 길어질 수 있다.
			     (3) 사용 편의성 >> 모델 서빙을 직접 관리하고, 메모리 사용과 배치 처리를 직접 최적화해야했다.
	

			2. vLLM을 사용한 경우
			     (0) 성능
				: 연속적인 배치 처리와 GPU 자원 최적화를 통해 처리량이 크게 향상되어, 입력된 텍스트에 대한 모델 추론이 더 빠르게 이루어졌다.=

			     (1) 메모리 효율성 >> vLLM은 GPU 메모리 사용을 최적화해준다. 특히, GPU와 CPU 블록의 최적화를 통해 메모리 사용이 최적화된 것을 확인 가능.


			     (2) 지연 시간 Latency >>vLLM은 배치 처리와 CUDA 그래프 최적화 등의 기능을 통해 지연 시간을 단축한다.실습에서는 텍스트 생성이 더 빠르게 이루어졌으며, 처리 속도가 크게 향상되었다.

			     (3) 사용 편의성 >> vLLM은 이러한 복잡한 작업들을 자동으로 처리해 주었으며, 사용자는 간단한 API 호출만으로 대규모 모델을 효율적으로 서빙 가능했다.









		(1) vLLM을 사용하지 않은 스크립트의 실행결과


(myenv) user@workstation:~/myenv/vllm_project$  python3 example.py
config.json: 100%|███████████████████████████████████████████████████████████████████████████████████| 665/665 [00:00<00:00, 9.06MB/s]
model.safetensors: 100%|███████████████████████████████████████████████████████████████████████████| 548M/548M [00:48<00:00, 11.3MB/s]
generation_config.json: 100%|████████████████████████████████████████████████████████████████████████| 124/124 [00:00<00:00, 1.43MB/s]
tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████████████| 26.0/26.0 [00:00<00:00, 343kB/s]
vocab.json: 100%|████████████████████████████████████████████████████████████████████████████████| 1.04M/1.04M [00:00<00:00, 9.86MB/s]
merges.txt: 100%|███████████████████████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 797kB/s]
tokenizer.json: 100%|████████████████████████████████████████████████████████████████████████████| 1.36M/1.36M [00:00<00:00, 1.44MB/s]
/home/user/myenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[{'generated_text': "The future of AI is not clear, as AI may evolve to address specific problems.\n\nAnswering my questions is more complex and time consuming. First, there are more than 140 questions to ponder like 'If I put the program 'On"}]


		     : 분석해보면 >> 
			1. config.json, model.safetensors, tokenizer.json 등의 파일이 다운로드 됨
				:  이 파일들은 GPT-2 모델과 관련된 설정 및 가중치 파일, 토크나이저 파일들로, GPT-2 모델을 처음 로드한거라 다운로드 된거다
			2. warning 출력 : 뭐 실행에 딱히 영향을 주진 않았으나 조심할 필요는 있다
			3. 텍스트 생성 결과 출력 >> 
				GPT-2 모델이 "The future of AI is"라는 입력 텍스트를 기반으로 아래의 결과를 반환했음을 확인 가능
					 The future of AI is not clear, as AI may evolve to address specific problems.
					 Answering my questions is more complex and time consuming. First, there are more than 140 questions to ponder like 'If I put the program 'On...



		(2) vLLM을 사용한 스크립트의 실행결과
(myenv) user@workstation:~/myenv/vllm_project$ python3 example.py 
/home/user/myenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
INFO 08-26 18:18:05 config.py:1559] Downcasting torch.float32 to torch.float16.
INFO 08-26 18:18:05 llm_engine.py:184] Initializing an LLM engine (v0.5.5) with config: model='gpt2', speculative_config=None, tokenizer='gpt2', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=gpt2, use_v2_block_manager=False, enable_prefix_caching=False)
INFO 08-26 18:18:06 model_runner.py:879] Starting to load model gpt2...
INFO 08-26 18:18:07 weight_utils.py:236] Using model weights format ['*.safetensors']
INFO 08-26 18:18:08 weight_utils.py:280] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 12.08it/s]

INFO 08-26 18:18:08 model_runner.py:890] Loading model weights took 0.2378 GB
INFO 08-26 18:18:08 gpu_executor.py:121] # GPU blocks: 17469, # CPU blocks: 7281
Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.41it/s, est. speed input: 67.08 toks/s, output: 214.64 toks/s]
 under attack. More and more companies are creating products and services that enable developers to




------------------------------------------------------------------------------------------------------

<종합 실습>

발급한 groq api key >> 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

Docker를 사용할 경우 따로 파이썬 가상환경을 생성할 필요가 없음 >> Docker  자체가 일종의 "가상환경" 역할을 하기 때문


개고생 원인
지금까지 여러 번의 수정과 테스트를 거치면서 겪은 어려움의 원인을 종합적으로 설명하자면 다음과 같습니다:

### 1. **버전 호환성 문제**
   - `transformers` 라이브러리의 특정 버전이나 `vLLM`의 특정 버전에서 사용되는 함수나 인자의 형식이 변경되었거나, 새로운 메서드가 도입되면서 예기치 않은 오류가 발생했습니다. 특히, `assert_never` 오류는 `vLLM`에서 처리되지 않은 예외적인 입력 형식 때문이었고, 이는 API 사용 방법이 변경된 것에 기인한 문제였습니다.

### 2. **잘못된 코드 구현**
   - 코드에서 `generate` 메서드를 호출할 때 잘못된 인자(`inputs`)를 사용하려고 하거나, `prompt_token_ids`를 잘못된 형식으로 전달하려고 했던 점이 있었습니다. 이로 인해 `TypeError` 또는 `AssertionError`와 같은 오류가 발생했습니다. 이는 `vLLM`의 API 사용 방식에 대한 오해 또는 잘못된 문서 참고에서 비롯되었습니다.

### 3. **모델 설정 및 초기화**
   - `vLLM` 라이브러리를 사용해 모델을 초기화하는 과정에서, GPU 메모리 관리와 CUDA 그래프 관련 설정이 제대로 이해되지 않아 초기화가 실패하거나 메모리 관련 문제를 일으켰습니다. 예를 들어, `enforce_eager=True`를 설정하지 않았을 때 CUDA 그래프 관련 경고가 발생했으며, 이로 인해 모델 로드 시 불필요한 메모리 사용이 발생할 수 있었습니다.

### 4. **로그 확인 및 디버깅 부족**
   - 초기 단계에서 로그를 충분히 활용하지 못해, 오류의 원인을 신속하게 파악하지 못했습니다. 이는 디버깅 속도를 저하시켰고, 여러 번의 시도 끝에야 문제의 본질을 이해할 수 있었습니다.

### 5. **도커 환경 문제**
   - 도커 컨테이너 내부에서 실행되는 환경과 로컬 개발 환경 간의 차이로 인해 예상치 못한 문제가 발생했습니다. 예를 들어, 모델 파일 로드, GPU 메모리 할당 등의 이슈가 컨테이너 환경에서 발생할 수 있었습니다.

### 종합적인 결론
이 모든 문제들은 대부분 라이브러리의 최신 버전에 대한 이해 부족, 새로운 API와의 호환성 문제, 그리고 로그 확인의 부족에서 비롯되었습니다. 결과적으로 여러 번의 수정과 테스트를 거치며 점진적으로 문제를 해결해 나갔고, 이제는 모델이 정상적으로 동작하는 상태에 도달할 수 있었습니다.

이러한 과정을 통해 얻은 교훈은, **문서화된 내용을 꼼꼼히 확인하고, 코드의 각 부분이 의도한 대로 동작하는지 세밀하게 확인하는 것**이 중요하다는 것입니다. 또한, **로그를 적극적으로 활용하여 문제 발생 시 빠르게 원인을 파악하는 것이 필요**합니다.


실습 내용

	(1) 스프링부트 프로젝트 작성
	    : dependency
		Spring Web: REST API를 구현하기 위해 필요
		Spring Boot DevTools: 개발 도중 빠른 재시작을 지원
		Lombok: 코드 간소화
		Spring Security: API 보호를 위해 필요

	    : 구조
	    : 기능
		(1) 텍스트 처리 ai 모델 api 적용 가능하게 만듦
		(2) 주요 애플리케이션에서 사용자의 AI 모델 API 호출량을 집계하는 기능
			: 두 애플리케이션 간의 연동이 필요
			: 



	(2) 서버 환경 구성
		: 구조
		

	(3) vLLM을 컨테이너화하고 Hugging Face 모델을 서빙


		0. Docker가 GPU를 사용가능하게 환경 조성
			(1) NVIDIA Docker가 설치되어있는지 확인 >>  sudo docker run --gpus all nvidia/cuda:11.0-base nvidia-smi 했을때 잘 실행되면 NVIDIA Docker가 올바르게 설치된 것
				: 근데 설치 안되있어서 아래와 같이 나옴
user@workstation:/var/www/myapp/deployments/v3.0.0/vLLM$ sudo docker run --gpus all nvidia/cuda:11.0-base nvidia-smi
Unable to find image 'nvidia/cuda:11.0-base' locally
docker: Error response from daemon: manifest for nvidia/cuda:11.0-base not found: manifest unknown: manifest unknown.
See 'docker run --help'.


			(2) ( NVIDIA Docker가 설치안된 경우) NVIDIA Docker 설치하기
				1. NVIDIA Container Toolkit 설치
distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \
&& curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \
&& curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list


				2. 패키지 업데이트 및 설치
sudo apt-get update
sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker

				3. NVIDIA Docker 테스트 >> sudo docker run --gpus all nvidia/cuda:12.2.0-runtime-ubuntu22.04 nvidia-smi
					:최신 버전의 이미지(CUDA 12.2 이미지로 시도)를 사용
					: 처음엔 Unable to find image 'nvidia/cuda:12.2.0-runtime-ubuntu22.04' locally 뜨고 나서 바로 이미지 다운로드 시작하니까 걱정 ㄴㄴ 
					: 다운로드 다 끝나면 아래와 같이 출력됨
						: 요약하자면 
							CUDA 버전: 12.2.0이 제대로 로드됨
							NVIDIA 드라이버 버전: 560.35.03 드라이버가 로드됨
							GPU 정보: NVIDIA GeForce RTX 4070이 정상적으로 인식되었고, GPU 메모리 사용량과 온도 등도 표시됨

user@workstation:/var/www/myapp/deployments/v3.0.0/vLLM$ sudo docker run --gpus all nvidia/cuda:12.2.0-runtime-ubuntu22.04 nvidia-smi
Unable to find image 'nvidia/cuda:12.2.0-runtime-ubuntu22.04' locally
12.2.0-runtime-ubuntu22.04: Pulling from nvidia/cuda
aece8493d397: Already exists 
9fe5ccccae45: Pull complete 
8054e9d6e8d6: Pull complete 
bdddd5cb92f6: Pull complete 
5324914b4472: Pull complete 
9a9dd462fc4c: Pull complete 
95eef45e00fa: Pull complete 
e2554c2d377e: Pull complete 
4640d022dbb8: Pull complete 
Digest: sha256:739e0bde7bafdb2ed9057865f53085539f51cbf8bd6bf719f2e114bab321e70e
Status: Downloaded newer image for nvidia/cuda:12.2.0-runtime-ubuntu22.04

==========
== CUDA ==
==========

CUDA Version 12.2.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Tue Aug 27 04:37:29 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 4070        Off |   00000000:01:00.0 Off |                  N/A |
|  0%   43C    P8             12W /  200W |      49MiB /  12282MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+


				


			(3) Docker Daemon 설정 확인
				1. /etc/docker/daemon.json 파일을 다음과 같이 수정 ("default-runtime": "nvidia"를 추가)

user@workstation:/etc/docker$ cat daemon.json 
{
    "default-runtime": "nvidia",
    "runtimes": {
        "nvidia": {
            "path": "nvidia-container-runtime",
            "runtimeArgs": []
        }
    }
}


				2.  Docker를 재시작 >> sudo systemctl restart docker







		1. Dockerfile 작성 >> 이미지 빌드를 위함
		    : 주요 내용
			(1)  컨테이너가 8081 포트에서 실행되도록 설정 <- EXPOSE 8081
			(2)   requirements.txt 를 참고하여 필요한 라이브러리 install 시킴 <- RUN pip3 install --no-cache-dir -r requirements.txt
			(3)  serve.py 이 실행되도록함 <- CMD ["python3", "serve.py"]


# Base image with CUDA and PyTorch
FROM nvidia/cuda:11.8.0-base-ubuntu22.04

# Install dependencies
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    git \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Upgrade pip
RUN pip3 install --upgrade pip

# Install Python packages
RUN pip3 install torch torchvision torchaudio \
    transformers flask vllm

# Copy serve.py to the container
COPY serve.py /app/serve.py

# Set working directory
WORKDIR /app

# Expose the port
EXPOSE 8081

# Command to run the server
CMD ["python3", "serve.py"]




		2. requirements.txt 작성 << vLLM과 Hugging Face Transformers 라이브러리를 설치하는데 필요한 dependency를 정의
vllm
transformers
torch
Flask>=2.2.0
Werkzeug>=2.2.

		3. serve.py 코드 작성 << 컨테이너 실행시 실행시킬, vLLM을 통해 Hugging Face 모델을 서빙하는 서버 코드. 
			



from flask import Flask, request, jsonify
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams
from prometheus_client import start_http_server, Summary, Counter, generate_latest

app = Flask(__name__)

# Prometheus metrics
REQUEST_TIME = Summary('request_processing_seconds', 'Time spent processing request')
REQUEST_COUNT = Counter('request_count', 'Total number of requests')

# Load model and tokenizer
model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
llm = LLM(model=model_name)

@app.route("/complete", methods=["POST"])
@REQUEST_TIME.time()  # Measure the time spent in this request
def complete():
    REQUEST_COUNT.inc()  # Increment the request count

    data = request.json
    prompt = data.get("prompt")
    max_tokens = data.get("max_tokens", 50)

    # Encode input text
    inputs = tokenizer(prompt, return_tensors="pt")

    # Convert tokens to list
    input_token_ids = inputs["input_ids"].tolist()[0]

    # Set sampling parameters
    sampling_params = SamplingParams(max_tokens=max_tokens, temperature=0.7, top_p=0.9)

    # Generate text using vLLM
    output = llm.generate(
        prompt_token_ids=[input_token_ids], 
        sampling_params=sampling_params, 
        use_tqdm=False
    )

    # Decode the generated sequences
    output_text = tokenizer.decode(output[0].outputs[0].token_ids, skip_special_tokens=True)

    return jsonify({"generated_text": output_text})

@app.route("/metrics")
def metrics():
    return generate_latest()

if __name__ == "__main__":
    # Start the Prometheus metrics server on port 8000
    start_http_server(8000)
    
    # Start the Flask application
    app.run(host="0.0.0.0", port=8081)




		4. 이미지 빌드 >> docker build -t vllm-gpt3-server .
			:  vllm-gpt3-server 라는 이름의 이미지 빌드함.

		5. Docker 컨테이너 실행 >> sudo docker run --gpus all -d -p 8081:8081 vllm-gpt3-server



	(4) 전반적인 실행 방법 << groq model 은 직접 서버에 올린게 아니라 별도의 작업은 필요 없고, vLLM 의 경우 컨테이너 띄워서 해야되는 거라 컨테이너 먼저 실행시켜야됨

`		1.  vLLM 컨테이너 실행시켜놓고 
		2. 스프링부트 애플리케이션 실행 
			: 로컬이든 서버에서든 뭐 상관없음
		3. vLLM에 대한 requewst

curl -X POST http://localhost:8081/complete -H "Content-Type: application/json" -d '{"prompt": "Once upon a time"}'



<< 스프링부트 빌드 및 배포>>
0. 	implementation group: 'net.andreinc', name: 'mockneat', version: '0.4.8' 를 활용하여 1000개의 더미 데이터 생성
  

1. gradlew.bat bootJar

2. scp ai01-0.0.1-SNAPSHOT.jar user@202.31.200.130:/var/www/myapp/deployments/v3.0.0/spring_boot/ai01-0.0.4-SNAPSHOT.jar
	
3. Dockerfile << 이미지 빌드용
# Use an official OpenJDK runtime as a parent image
FROM openjdk:21

# Set the working directory in the container
WORKDIR /app

# Copy the application .jar file to the container
COPY ai01-0.0.4-SNAPSHOT.jar /app/ai01.jar

# Specify the command to run the application
ENTRYPOINT ["java","-jar","/app/ai01.jar"]



4. 이미지 빌드 >> sudo docker build -t ai01-app:0.0.1 .



<<prometheus>> 


1. prometheus/prometheus.yml 을 아래처럼 작성

global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'spring-boot'
    scrape_interval: 5s
    metrics_path: '/actuator/prometheus'
    static_configs:
      - targets: ['nginx:80']  

  - job_name: 'vllm'
    static_configs:
      - targets: ['vllm:8081']




<<nginx/default.conf>>
server {
    listen 80;

    location / {
        proxy_pass http://spring_boot:8080;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }

    location /vllm {
        proxy_pass http://vllm:8081;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}



<< docker-compose.yml >>

: 작성은 아래처럼

version: '3.8'

services:
  spring_boot:
    build:
      context: ./spring_boot
    image: spring_boot_app
    ports:
      - "8080:8080"
    networks:
      - monitoring

  vllm:
    build:
      context: ./vLLM
    image: vllm_app
    ports:
      - "8081:8081"
    networks:
      - monitoring

  prometheus:
    image: prom/prometheus
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
    networks:
      - monitoring

  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-storage:/var/lib/grafana
    depends_on:
      - prometheus
    networks:
      - monitoring

  nginx:
    image: nginx:latest
    volumes:
      - ./nginx/default.conf:/etc/nginx/conf.d/default.conf
    ports:
      - "80:80"
    depends_on:
      - spring_boot
      - vllm
    networks:
      - monitoring

networks:
  monitoring:
    driver: bridge

volumes:
  grafana-storage:



실행은 이걸로 >> docker-compose up --build


http_server_requests_seconds_count{user_id="specific_user_id"}




